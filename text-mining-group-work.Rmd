---
title: "text-mining-group-work"
author: "Vladyslav Gorbunov, Larissa Pagliarin"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: "hpstr" 
  html_document:
    toc: true
    toc_float: true
---

## Data Import and Preparation

We Import the dataset *SP500_data.csv* and make a copy to work with it and named it *data*. We copy it so we can be secure that i do not make any changes in the original dataset.
<br>
We use several libraries to process the tasks and get the output that is asked.


```{r library, include=FALSE}
# Load packages and libraries, if not already available.
libraries = c("ggplot2", "knitr", "dplyr", "tidyverse", "lubridate", "tm", "wordcloud", "SnowballC")

lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r data, include=FALSE}
# Load data set and make a copy of the original
options(scipen=999)
Tweets_all <- load("Tweets_all.rda")
```

<br>

## Data Exploration

This section gives a concise view of the Tweets on the Swiss Univerity Social Media accounts data.
<br>
The dataset consists 19'575 observations and 14 variables:
<br>
<br>

**Time Range and Tweet Frequency:**
<br>

- Tweets are from September 29, 2009, to January 26, 2023 and this indicates a long-term use of Twitter
- The median tweet date is April 13, 2018, suggesting that half of the tweets were posted after this date and the data is skewed
<br>

*Retweet and Favorite Counts:**
<br>

- The data shows a minimum of 0 and a maximum of 267 retweets and 188 likes per tweet
- the median and first quartile for retweets and likes are 0, indicating that many tweets receive little to no engagement
- The `in_reply_to_screen_name` field suggests that some tweets are responses to other users, which might indicate engagement or conversation strategies used by the university
<br>

**ID and String Variables:**
<br>
- The `id` and `id_str` fields are technical identifiers for tweets, indicating that tweets have been collected over a wide range of tweet
<br>

**Language and University Fields:**
<br>

- The `lang` shows the common language used at the university 
- `university` shows the abbreviation of the university
<br>

**Temporal Patterns:**
<br>

- `created_at`, `tweet_date`, `tweet_hour`, and `tweet_month` provide detailed temporal data
- can be analyzed to understand peak times of activity and seasonal or monthly trends in tweeting behavior.
<br>

**Content Analysis**
<br>

The word cloud represents the most frequently used words in the filtered tweets with high engagement (likes or retweets). Key observations include:

Frequent Terms: Larger words such as "bachelor," "design," "die," "das," "der," and "amp" indicate their higher occurrence.
Key Topics:
"bachelor" for Bachelor's programs or graduates.
"design" related to design courses or projects.
"HSLU" (Hochschule Luzern).
General terms: "schweiz," "zeigen," "nicht."
Note: The term "amp" appears due to HTML encoding and is not meaningful.


<br>
```{r}
head(tweets)
summary(tweets)
```





### Data Preprocessing

```{r}
# Corpus: Collection of text documents that generally serves as a basis for analysis in text processing and text mining.
# VectorSource(tweets): This vector is then used as the source for the corpus, whereby each entry in the vector becomes a separate document in the corpus.
corpus <- Corpus(VectorSource(tweets$full_text))
```


```{r}
#Replace special characters with blank space, if there are any using gsub
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/")
corpus <- tm_map(corpus, toSpace, "@")
corpus <- tm_map(corpus, toSpace, "\\|")
```
```{r}
# Convert the text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
# Text stemming (remove suffixes, etc.; only root form of the word)
#corpus <- tm_map(corpus, stemDocument) #doesn't work that nicely
```


```{r}
# Code to identify and possibly filter out retweets and automated tweets
tweets <- tweets %>%
  filter(!is.na(full_text))  # Ensures only tweets with text are analyzed
```


```{r}
summary(tweets)
```

### Tweet Frequency Analysis

```{r}
# Code to analyze tweet frequencies by time and university
tweets %>%
  mutate(tweet_month = floor_date(created_at, "month")) %>%
  group_by(university, tweet_month) %>%
  summarize(count = n(), .groups = 'drop') %>%
  ggplot(aes(x = tweet_month, y = count, fill = university)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  labs(title = "Monthly Tweet Frequency by University", x = "Month", y = "Number of Tweets")
```
### Content Analysis

```{r}
### Content Analysis

# Set a threshold for "high engagement" (e.g., tweets with at least 10 likes or retweets)
engagement_threshold <- 10

# Filter tweets based on this engagement threshold
high_engagement_tweets <- tweets %>%
  filter(favorite_count >= engagement_threshold | retweet_count >= engagement_threshold)

# Rebuild the corpus with the sampled data
corpus <- Corpus(VectorSource(tweets_sample$full_text))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Create DTM and remove sparse terms
dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = TRUE, stopwords = TRUE, wordLengths = c(1, Inf)))
dtm <- removeSparseTerms(dtm, sparse = 0.99)  # Adjust sparsity threshold as needed

# Ensure word names are captured
word_freq <- sort(rowSums(as.matrix(dtm)), decreasing = TRUE)
top_word_freq <- head(word_freq, 25)
word_names <- colnames(dtm)

# Generate word cloud using the correct word names
wordcloud(
  words = word_names, 
  freq = top_word_freq, 
  max.words = 25,
  scale = c(4, 0.5),       # Control for size of the most and least frequent words
  random.order = FALSE,    # Higher frequency words appear first
  rot.per = 0.25,          # Allows some rotation for fitting
  colors = brewer.pal(8, "Dark2")  # Enhances visual appeal
)
```


### Engagement Analysis

```{r}
# Analysis of likes and retweets
tweets %>%
  group_by(university) %>%
  summarize(total_likes = sum(favorite_count), total_retweets = sum(retweet_count), .groups = 'drop') %>%
  ggplot(aes(x = reorder(university, total_likes), y = total_likes)) +
  geom_col() +
  coord_flip() +
  labs(title = "Engagement Analysis by University", x = "University", y = "Total Likes")

```



## Recommendations




## Conclusion
























